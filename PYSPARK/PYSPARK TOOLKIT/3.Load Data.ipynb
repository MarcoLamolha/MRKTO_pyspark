{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYSPARK TOOLKIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n",
      "3.5.3\n",
      "PySparkSQL\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkSQL\").getOrCreate()\n",
    "\n",
    "# Spark: Check Session and Print Details\n",
    "print(type(spark))\n",
    "print(spark.version)\n",
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" pyspark.sql.DataFrameReader\n",
    "\n",
    "> Official documentation:\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html\n",
    "\"\"\"\n",
    "\n",
    "# Load data into a DataFrame from a CSV file\n",
    "df_csv = spark.read.csv(r\"C:\\Users\\marco.SENNA-LAMOLHA\\OneDrive\\Documentos\\PORTFOLIO\\PYTHON\\MRKTO_python\\PYSPARK\\DATASETS\\data_csv.csv\", header=True, inferSchema=True)\n",
    "df_csv = spark.read.format(\"csv\").load(r\"C:\\Users\\marco.SENNA-LAMOLHA\\OneDrive\\Documentos\\PORTFOLIO\\PYTHON\\MRKTO_python\\PYSPARK\\DATASETS\\data_csv.csv\", header=True, inferSchema=True)  # Alternative way\n",
    "\n",
    "\n",
    "# Load data into a DataFrame from different data formats\n",
    "df_json = spark.read.json()\n",
    "df_parquet = spark.read.parquet()\n",
    "\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DataFrame Operations\n",
    "Perform basic operations on DataFrames such as selecting columns, filtering rows, and aggregating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"column1\", \"column2\").show()\n",
    "\n",
    "# Filter rows based on a condition\n",
    "df.filter(df[\"column1\"] > 100).show()\n",
    "\n",
    "# Group by a column and aggregate data\n",
    "df.groupBy(\"column3\").count().show()\n",
    "\n",
    "# Perform a SQL query on the DataFrame\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "spark.sql(\"SELECT column1, COUNT(*) FROM table GROUP BY column1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, the \n",
    "\n",
    "parallelize\n",
    "\n",
    " method is used to create an RDD (Resilient Distributed Dataset) from a Python collection, such as a list or a set. RDDs are the fundamental data structure of Apache Spark, representing a distributed collection of objects that can be processed in parallel across a cluster of machines. The \n",
    "\n",
    "parallelize\n",
    "\n",
    " method is particularly useful when you want to distribute a local collection of data across the Spark cluster to leverage the parallel processing capabilities of Spark.\n",
    "\n",
    "Here's a simple example to illustrate the use of \n",
    "\n",
    "parallelize\n",
    "\n",
    ":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DataFrame from an RDD\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)])\n",
    "df = spark.createDataFrame(rdd, [\"Name\", \"ID\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of numbers\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Parallelize the list to create an RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform an action on the RDD (e.g., collect the elements)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('Company name',df['Company name'].cast('string')).show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries on DataFrame\n",
    "Use SQL queries to interact with DataFrames by registering them as temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "\n",
    "# Perform a SQL query to select specific columns\n",
    "result = spark.sql(\"SELECT column1, column2 FROM table\")\n",
    "result.show()\n",
    "\n",
    "# Perform a SQL query to filter rows based on a condition\n",
    "result = spark.sql(\"SELECT * FROM table WHERE column1 > 100\")\n",
    "result.show()\n",
    "\n",
    "# Perform a SQL query to group by a column and aggregate data\n",
    "result = spark.sql(\"SELECT column3, COUNT(*) as count FROM table GROUP BY column3\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Transformations\n",
    "Apply transformations to DataFrames such as map, filter, and groupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Transformations\n",
    "\n",
    "# Apply a map transformation to a DataFrame\n",
    "mapped_df = df.rdd.map(lambda row: (row.column1, row.column2 * 2)).toDF([\"column1\", \"column2_transformed\"])\n",
    "mapped_df.show()\n",
    "\n",
    "# Apply a filter transformation to a DataFrame\n",
    "filtered_df = df.filter(df[\"column1\"] > 100)\n",
    "filtered_df.show()\n",
    "\n",
    "# Apply a groupBy transformation to a DataFrame and aggregate data\n",
    "grouped_df = df.groupBy(\"column3\").count()\n",
    "grouped_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
